{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x20b9fefefd0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.optim import lr_scheduler\n",
    "import torchtext\n",
    "import contractions\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "torch.manual_seed(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = './data/train'\n",
    "dev_file = './data/dev'\n",
    "test_file = './data/test'\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions to read data files and create sentences and tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_data(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "\n",
    "    sentences = []\n",
    "    sentence_tags = []\n",
    "    current_sentence = []\n",
    "    current_tags = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line == '':\n",
    "            sentences.append(current_sentence)\n",
    "            sentence_tags.append(current_tags)\n",
    "            current_sentence = []\n",
    "            current_tags = []\n",
    "            continue\n",
    "        [index, word, tag] = line.split(' ')\n",
    "        current_sentence.append(word)\n",
    "        current_tags.append(tag)\n",
    "    sentences.append(current_sentence)\n",
    "    sentence_tags.append(current_tags)\n",
    "    return sentences, sentence_tags\n",
    "\n",
    "def read_test_data(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "        \n",
    "    sentences = []\n",
    "    current_sentence = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line == '':\n",
    "            sentences.append(current_sentence)\n",
    "            current_sentence = []\n",
    "            continue\n",
    "        [index, word] = line.split(' ')\n",
    "        current_sentence.append(word)\n",
    "    sentences.append(current_sentence)\n",
    "    return sentences    \n",
    "\n",
    "train_sentences, train_sentence_tags = read_train_data(train_file)        \n",
    "dev_sentences, dev_sentence_tags = read_train_data(dev_file)\n",
    "test_sentences = read_test_data(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset and train helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for DataLoader\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.lengths = [len(x) for x in self.X]\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.lengths[idx]\n",
    "\n",
    "def pad_collate(batch):\n",
    "    (xx, yy, lengths) = zip(*batch)\n",
    "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=0).type(torch.LongTensor).to(device)\n",
    "    yy_pad = pad_sequence(yy, batch_first=True, padding_value=-1).type(torch.LongTensor).to(device)\n",
    "    return xx_pad, yy_pad, lengths   \n",
    "\n",
    "# Train function\n",
    "def train(model, train_data, optimizer, criterion):\n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_data):\n",
    "        sentences, tags, lengths = batch\n",
    "        optimizer.zero_grad()\n",
    "        output = model(sentences, lengths)\n",
    "        loss = criterion(output.view(-1, output.shape[-1]), tags.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction and Evaluation Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, eval_data, batch_size):\n",
    "    predicted_tags = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(eval_data), batch_size):\n",
    "            batch = eval_data[i:i+batch_size]\n",
    "            lengths = [len(x) for x in batch]\n",
    "            sentences = pad_sequence(batch, batch_first=True, padding_value=0).type(torch.LongTensor).to(device)\n",
    "            output = model(sentences, lengths)\n",
    "            preds = output.argmax(dim=-1)\n",
    "            for idx, sentence in enumerate(batch):\n",
    "                pad_index = len(sentence)\n",
    "                predicted_tags.append(preds[idx, 0: pad_index])\n",
    "    return predicted_tags\n",
    "\n",
    "def get_predicted_sentence_tags(preds, labels_vocab):\n",
    "    itos = labels_vocab.get_itos()\n",
    "    sentence_tags = []\n",
    "    for i in range(len(preds)):\n",
    "        current_sentence_tags = []\n",
    "        j = 0\n",
    "        while j<len(preds[i]):\n",
    "            current_sentence_tags.append(itos[int(preds[i][j])])\n",
    "            j += 1\n",
    "        sentence_tags.append(current_sentence_tags)\n",
    "    return sentence_tags\n",
    "\n",
    "def write_test_output(filename, sentences, sentence_tags, predicted_tags):\n",
    "    with open(filename, 'w') as f:\n",
    "        s = \"\"\n",
    "        for i in range(len(sentences)):\n",
    "            assert len(sentences[i]) == len(predicted_tags[i])\n",
    "            if i != 0:\n",
    "                s += '\\n'\n",
    "            for j in range(len(sentences[i])):\n",
    "                s += \"{} {} {} {}\\n\".format(j+1, sentences[i][j], sentence_tags[i][j], predicted_tags[i][j])\n",
    "        f.write(s)\n",
    "        f.close()\n",
    "        \n",
    "def calculate_score(true_tags, predicted_tags):\n",
    "    true_tags = list(np.concatenate(true_tags))\n",
    "    predicted_tags = list(np.concatenate(predicted_tags))\n",
    "    total = len(predicted_tags)\n",
    "    matched = 0\n",
    "    assert(len(predicted_tags) == len(true_tags))\n",
    "    for i in range(len(predicted_tags)):\n",
    "        if predicted_tags[i] == true_tags[i]:\n",
    "            matched += 1\n",
    "    return matched/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Simple BLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create numerical sentences with torchtext vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(sentences, vocab, is_X):\n",
    "    joined_sentences = [' '.join(sentence) for sentence in sentences]\n",
    "    # Tokenize sentences\n",
    "    tokenizer = torchtext.data.utils.get_tokenizer(None)\n",
    "    tokenized_sentences = [tokenizer(sentence) for sentence in joined_sentences]\n",
    "    if vocab is None:\n",
    "        if is_X:\n",
    "            vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "                tokenized_sentences,\n",
    "                specials=[\"<pad>\", \"<unk>\"],\n",
    "                min_freq=2,\n",
    "                )\n",
    "            vocab.set_default_index(vocab[\"<unk>\"])\n",
    "        else:\n",
    "            vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_sentences)\n",
    "    # Convert tokenized sentences to numerical indices\n",
    "    numerical_sentences = [torch.tensor(vocab.lookup_indices(sentence)).to(torch.int) for sentence in tokenized_sentences]\n",
    "    return vocab, numerical_sentences\n",
    "\n",
    "vocab, train_X = create_vocab(train_sentences, vocab=None, is_X=True)\n",
    "labels_vocab, train_Y = create_vocab(train_sentence_tags, vocab=None, is_X=False)\n",
    "\n",
    "dev_vocab, dev_X = create_vocab(dev_sentences, vocab=vocab, is_X=True)\n",
    "dev_labels_vocab, dev_Y = create_vocab(dev_sentence_tags, vocab=labels_vocab, is_X=False)\n",
    "\n",
    "test_vocab, test_X = create_vocab(test_sentences, vocab=vocab, is_X=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim*2, hidden_dim)\n",
    "        self.elu = nn.ELU()\n",
    "        self.out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(0.33)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # text shape: (batch_size, seq_length)\n",
    "        embedded = self.embedding(x)  # embedded shape: (batch_size, seq_length, embedding_dim)\n",
    "        lstm_output = pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "        lstm_output, _ = self.lstm(lstm_output) # lstm_output shape: (batch_size, seq_length, hidden_dim*2)\n",
    "        lstm_output, _ = pad_packed_sequence(lstm_output, batch_first=True)\n",
    "        lstm_output = self.dropout(lstm_output)\n",
    "        linear_output = self.fc(lstm_output)  # linear_output shape: (batch_size, seq_length, hidden_dim)\n",
    "        elu_output = self.elu(linear_output)  # elu_output shape: (batch_size, seq_length, hidden_dim)\n",
    "        elu_output = self.dropout(elu_output)\n",
    "        output = self.out(elu_output)  # tag_space shape: (batch_size, seq_length, output_dim)\n",
    "        return output      \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 100\n",
    "num_layers = 1\n",
    "hidden_dim = 256\n",
    "output_dim = len(labels_vocab)\n",
    "lr = 1\n",
    "batch_size = 64\n",
    "num_epochs = 40\n",
    "train_dataset = NERDataset(train_X, train_Y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
    "val_dataset = NERDataset(dev_X, dev_Y)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task1_model_path = './blstm1.pt' \n",
    "if not os.path.exists(task1_model_path):\n",
    "    # Create model and optimizer\n",
    "    model = NERModel(vocab_size, embedding_dim, hidden_dim, num_layers, output_dim)\n",
    "    model.to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train(model, train_loader, optimizer, criterion)\n",
    "        predicted_train_tags = get_predicted_sentence_tags(predict(model, train_X, batch_size), labels_vocab)\n",
    "        train_accuracy = calculate_score(train_sentence_tags, predicted_train_tags)\n",
    "        predicted_dev_tags = get_predicted_sentence_tags(predict(model, dev_X, batch_size), labels_vocab)\n",
    "        val_accuracy = calculate_score(dev_sentence_tags, predicted_dev_tags)\n",
    "        print(f'Epoch {epoch+1}: Train Accuracy={train_accuracy:.4f}')\n",
    "        print(f'Epoch {epoch+1}: Validation Accuracy={val_accuracy:.4f}')\n",
    "        scheduler.step()\n",
    "    \n",
    "    torch.save(model, task1_model_path)\n",
    "else:\n",
    "    model = torch.load(task1_model_path)\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model and Write prediction files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9566287952227694\n",
      "processed 51578 tokens with 5942 phrases; found: 5301 phrases; correct: 4426.\n",
      "accuracy:  95.66%; precision:  83.49%; recall:  74.49%; FB1:  78.73\n",
      "              LOC: precision:  91.05%; recall:  81.44%; FB1:  85.98  1643\n",
      "             MISC: precision:  85.31%; recall:  74.30%; FB1:  79.42  803\n",
      "              ORG: precision:  74.94%; recall:  68.01%; FB1:  71.31  1217\n",
      "              PER: precision:  81.38%; recall:  72.37%; FB1:  76.61  1638\n"
     ]
    }
   ],
   "source": [
    "predicted_dev_tags = get_predicted_sentence_tags(predict(model, dev_X, batch_size), labels_vocab)\n",
    "write_test_output('./dev.out', dev_sentences, dev_sentence_tags, predicted_dev_tags)\n",
    "print(calculate_score(dev_sentence_tags, predicted_dev_tags))\n",
    "!perl conll03eval.txt < dev.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: GloVe Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./glove.6B.100d.txt', encoding='utf-8') as f: \n",
    "    lines = f.readlines()\n",
    "    word_to_vec = {}\n",
    "    word_to_vec[\"<pad>\"] = np.concatenate((np.random.rand(100), np.zeros([1])))\n",
    "    unk_vec = np.random.rand(100)\n",
    "    word_to_vec[\"<unk>\"] = np.concatenate((unk_vec, np.zeros([1])))\n",
    "    word_to_vec[\"<UNK>\"] = np.concatenate((unk_vec, np.ones([1])))\n",
    "    for line in lines:\n",
    "        tokens = line.split()\n",
    "        vec = np.array([float(x) for x in tokens[1:]])\n",
    "        s = str(tokens[0])\n",
    "        lower_vec = np.concatenate((vec, np.zeros([1])))\n",
    "        upper_vec = np.concatenate((vec, np.ones([1])))\n",
    "        word_to_vec[s] = lower_vec\n",
    "        word_to_vec[s.upper()] = upper_vec\n",
    "\n",
    "vocab = list(word_to_vec.keys())\n",
    "assert(vocab.index(\"<pad>\") == 0)\n",
    "vocab_size = len(vocab)\n",
    "embeddings_weights = np.zeros([vocab_size, 101])\n",
    "for idx, key in enumerate(vocab):\n",
    "    embeddings_weights[idx] = word_to_vec[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_numerical_vectors(sentences, vocab):\n",
    "    vocab_to_index = {}\n",
    "    for idx, v in enumerate(vocab):\n",
    "        vocab_to_index[v] = idx\n",
    "    joined_sentences = [' '.join(sentence) for sentence in sentences]\n",
    "    # Tokenize sentences\n",
    "    tokenizer = torchtext.data.utils.get_tokenizer(None)\n",
    "    tokenized_sentences = [tokenizer(sentence) for sentence in joined_sentences]\n",
    "    numerical_sentences = []\n",
    "    vocab_set = set(vocab)\n",
    "    for sentence in tokenized_sentences:\n",
    "        vector = []\n",
    "        for token in sentence:\n",
    "            if token.lower() in vocab_set:\n",
    "                if token.lower() == token:\n",
    "                    vector.append(vocab_to_index[token])\n",
    "                else:\n",
    "                    vector.append(vocab_to_index[token.upper()])\n",
    "            else:\n",
    "                if token.lower() == token:\n",
    "                    vector.append(vocab_to_index[\"<unk>\"])\n",
    "                else:\n",
    "                    vector.append(vocab_to_index[\"<unk>\".upper()])\n",
    "        numerical_sentences.append(torch.tensor(vector).to(torch.int))\n",
    "    return numerical_sentences\n",
    "\n",
    "train_X = create_numerical_vectors(train_sentences, vocab)\n",
    "labels_vocab, train_Y = create_vocab(train_sentence_tags, vocab=None, is_X=False)\n",
    "\n",
    "dev_X = create_numerical_vectors(dev_sentences, vocab)\n",
    "dev_labels_vocab, dev_Y = create_vocab(dev_sentence_tags, vocab=labels_vocab, is_X=False)\n",
    "\n",
    "test_X = create_numerical_vectors(test_sentences, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloveNERModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, pretrained_weight, hidden_dim, num_layers, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(pretrained_weight))\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim*2, hidden_dim)\n",
    "        self.elu = nn.ELU()\n",
    "        self.out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(0.33)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # text shape: (batch_size, seq_length)\n",
    "        embedded = self.embedding(x)  # embedded shape: (batch_size, seq_length, embedding_dim)\n",
    "        lstm_output = pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "        lstm_output, _ = self.lstm(lstm_output) # lstm_output shape: (batch_size, seq_length, hidden_dim*2)\n",
    "        lstm_output, _ = pad_packed_sequence(lstm_output, batch_first=True)\n",
    "        lstm_output = self.dropout(lstm_output)\n",
    "        linear_output = self.fc(lstm_output)  # linear_output shape: (batch_size, seq_length, hidden_dim)\n",
    "        elu_output = self.elu(linear_output)  # elu_output shape: (batch_size, seq_length, hidden_dim)\n",
    "        elu_output = self.dropout(elu_output)\n",
    "        output = self.out(elu_output)  # tag_space shape: (batch_size, seq_length, output_dim)\n",
    "        return output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 101\n",
    "num_layers = 1\n",
    "hidden_dim = 256\n",
    "output_dim = len(labels_vocab)\n",
    "lr = 1\n",
    "batch_size = 64\n",
    "num_epochs = 40\n",
    "train_dataset = NERDataset(train_X, train_Y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
    "val_dataset = NERDataset(dev_X, dev_Y)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "task2_model_path = './blstm2.pt' \n",
    "if not os.path.exists(task2_model_path):\n",
    "    model = GloveNERModel(vocab_size, embedding_dim, embeddings_weights, hidden_dim, num_layers, output_dim)\n",
    "    model.to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train(model, train_loader, optimizer, criterion)\n",
    "        predicted_train_tags = get_predicted_sentence_tags(predict(model, train_X, batch_size), labels_vocab)\n",
    "        train_accuracy = calculate_score(train_sentence_tags, predicted_train_tags)\n",
    "        predicted_dev_tags = get_predicted_sentence_tags(predict(model, dev_X, batch_size), labels_vocab)\n",
    "        val_accuracy = calculate_score(dev_sentence_tags, predicted_dev_tags)\n",
    "        print(f'Epoch {epoch+1}: Train Accuracy={train_accuracy:.4f}')\n",
    "        print(f'Epoch {epoch+1}: Validation Accuracy={val_accuracy:.4f}')\n",
    "        scheduler.step()\n",
    "    \n",
    "    # save model to file\n",
    "    torch.save(model, task2_model_path)\n",
    "else:\n",
    "    model = torch.load(task2_model_path)\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9846640040327271\n",
      "processed 51578 tokens with 5942 phrases; found: 6062 phrases; correct: 5439.\n",
      "accuracy:  98.47%; precision:  89.72%; recall:  91.53%; FB1:  90.62\n",
      "              LOC: precision:  94.10%; recall:  94.72%; FB1:  94.41  1849\n",
      "             MISC: precision:  82.19%; recall:  84.60%; FB1:  83.38  949\n",
      "              ORG: precision:  83.37%; recall:  86.35%; FB1:  84.84  1389\n",
      "              PER: precision:  93.92%; recall:  95.60%; FB1:  94.75  1875\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, vocab_size, embeddings_weights = create_glove_vocab()\n",
    "train_X = create_numerical_vectors(train_sentences, vocab)\n",
    "labels_vocab, train_Y = create_vocab(train_sentence_tags, vocab=None, is_X=False)\n",
    "dev_X = create_numerical_vectors(dev_sentences, vocab)\n",
    "dev_labels_vocab, dev_Y = create_vocab(dev_sentence_tags, vocab=labels_vocab, is_X=False)\n",
    "test_X = create_numerical_vectors(test_sentences, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_char_vocab(sentences, vocab, max_word_length):\n",
    "    if max_word_length is None:\n",
    "        max_word_length = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if len(word) > max_word_length:\n",
    "                    max_word_length = len(word)\n",
    "                \n",
    "    if vocab is None:\n",
    "        char_frequencies = {}\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                for char in word:\n",
    "                    if char not in char_frequencies:\n",
    "                        char_frequencies[char] = 1\n",
    "                    else:\n",
    "                        char_frequencies[char] = char_frequencies[char] + 1\n",
    "                        \n",
    "        threshold = 2\n",
    "        keys = list(char_frequencies.keys())\n",
    "        for key in keys:\n",
    "            if char_frequencies[key] <= threshold:\n",
    "                del char_frequencies[key]\n",
    "        \n",
    "        vocab =  list(char_frequencies.keys())\n",
    "        vocab = [\"<pad>\", \"<unk>\"] + vocab\n",
    "\n",
    "    char_to_index = {}\n",
    "    for idx, char in enumerate(vocab):\n",
    "        char_to_index[char] = idx\n",
    "\n",
    "    numerical_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence_vec = []\n",
    "        for word in sentence:\n",
    "            word_tensor = torch.zeros([max_word_length])\n",
    "            word_vec = []\n",
    "            for char in word:\n",
    "                if char not in char_to_index:\n",
    "                    word_vec.append(char_to_index[\"<unk>\"])\n",
    "                else:\n",
    "                    word_vec.append(char_to_index[char])\n",
    "            word_tensor[0:len(word_vec)] = torch.tensor(word_vec).to(torch.int)\n",
    "            sentence_vec.append(word_tensor)\n",
    "        sentence_vec = pad_sequence(sentence_vec, batch_first=True, padding_value=0).type(torch.int).to(device)\n",
    "        numerical_sentences.append(sentence_vec)\n",
    "    return vocab, numerical_sentences, max_word_length\n",
    "\n",
    "\n",
    "char_vocab, char_train_X, max_word_length = create_char_vocab(train_sentences, None, None)\n",
    "_, char_dev_X, max_word_length = create_char_vocab(dev_sentences, char_vocab, max_word_length)\n",
    "_, char_test_X, max_word_length = create_char_vocab(test_sentences, char_vocab, max_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharCNN(nn.Module):\n",
    "    def __init__(self, char_vocab_size, char_embedding_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.char_embedding = nn.Embedding(char_vocab_size, char_embedding_dim, padding_idx=0)\n",
    "        self.conv1 = nn.Conv1d(char_embedding_dim, output_dim, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(output_dim, output_dim, kernel_size=3, padding=1)\n",
    "        self.elu = nn.ELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is a tensor of shape (batch_size, seq_len, max_word_length)\n",
    "        batch_size, seq_len, max_word_length = x.shape\n",
    "        x = x.view(-1, max_word_length)  # reshape to (batch_size * seq_len, max_word_length)\n",
    "        x = self.char_embedding(x)  # (batch_size * seq_len, max_word_length, char_embedding_dim)\n",
    "        x = x.transpose(1, 2)  # (batch_size * seq_len, char_embedding_dim, max_word_length)\n",
    "        x = self.elu(self.conv1(x))  # (batch_size * seq_len, output_dim, max_word_length)\n",
    "        x = self.elu(self.conv2(x))  # (batch_size * seq_len, output_dim, max_word_length)\n",
    "        x, _ = torch.max(x, dim=2)  # (batch_size * seq_len, output_dim)\n",
    "        x = x.view(batch_size, seq_len, -1)  # reshape to (batch_size, seq_len, output_dim)\n",
    "        return x\n",
    "\n",
    "class CharCNNLSTM(nn.Module):\n",
    "    def __init__(self, word_vocab_size, word_embedding_dim, pretrained_weight, char_vocab_size, char_embedding_dim, char_output_dim, hidden_dim, num_layers, linear_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.word_embedding = nn.Embedding(word_vocab_size, word_embedding_dim, padding_idx=0)\n",
    "        self.word_embedding.weight.data.copy_(torch.from_numpy(pretrained_weight))\n",
    "        self.char_cnn = CharCNN(char_vocab_size, char_embedding_dim, char_output_dim)\n",
    "        self.lstm = nn.LSTM(word_embedding_dim + char_output_dim, hidden_dim, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim*2, linear_dim)\n",
    "        self.elu = nn.ELU()\n",
    "        self.out = nn.Linear(linear_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(0.33)\n",
    "        \n",
    "    def forward(self, chars, words, lengths):\n",
    "        # words is a tensor of shape (batch_size, seq_len)\n",
    "        # chars is a tensor of shape (batch_size, seq_len, max_word_length)\n",
    "        word_embedded = self.word_embedding(words)  # (batch_size, seq_len, word_embedding_dim)\n",
    "        char_embedded = self.char_cnn(chars)  # (batch_size, seq_len, char_output_dim)\n",
    "        combined_embedded = torch.cat((word_embedded, char_embedded), dim=2)  # (batch_size, seq_len, word_embedding_dim + char_output_dim)\n",
    "        lstm_output = pack_padded_sequence(combined_embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "        lstm_output, _ = self.lstm(lstm_output) # lstm_output shape: (batch_size, seq_length, hidden_dim*2)\n",
    "        lstm_output, _ = pad_packed_sequence(lstm_output, batch_first=True)\n",
    "        lstm_output = self.dropout(lstm_output)\n",
    "        linear_output = self.fc(lstm_output)  # linear_output shape: (batch_size, seq_length, hidden_dim)\n",
    "        elu_output = self.elu(linear_output)  # elu_output shape: (batch_size, seq_length, hidden_dim)\n",
    "        elu_output = self.dropout(elu_output)\n",
    "        output = self.out(elu_output)\n",
    "        return output\n",
    "        \n",
    "\n",
    "# Dataset for DataLoader\n",
    "class CNNNERDataset(Dataset):\n",
    "    def __init__(self, char_X, X, y):\n",
    "        self.char_X = char_X\n",
    "        self.X = X\n",
    "        self.lengths = [len(x) for x in self.X]\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.char_X[idx], self.X[idx], self.y[idx], self.lengths[idx]\n",
    "\n",
    "def pad_collate(batch):\n",
    "    (char_xx, xx, yy, lengths) = zip(*batch)\n",
    "    char_xx = pad_sequence(char_xx, batch_first=True, padding_value=0).type(torch.LongTensor).to(device)\n",
    "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=0).type(torch.LongTensor).to(device)\n",
    "    yy_pad = pad_sequence(yy, batch_first=True, padding_value=-1).type(torch.LongTensor).to(device)\n",
    "    \n",
    "    return char_xx, xx_pad, yy_pad, lengths   \n",
    "\n",
    "# Train function\n",
    "def train(model, train_data, optimizer, criterion):\n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_data):\n",
    "        chars, sentences, tags, lengths = batch\n",
    "        optimizer.zero_grad()\n",
    "        output = model(chars, sentences, lengths)\n",
    "        loss = criterion(output.view(-1, output.shape[-1]), tags.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "def cnn_predict(model, eval_data, char_eval_data, batch_size):\n",
    "    predicted_tags = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(eval_data), batch_size):\n",
    "            batch = eval_data[i:i+batch_size]\n",
    "            char_batch = char_eval_data[i:i+batch_size]\n",
    "            lengths = [len(x) for x in batch]\n",
    "            sentences = pad_sequence(batch, batch_first=True, padding_value=0).type(torch.LongTensor).to(device)\n",
    "            char_sentences = pad_sequence(char_batch, batch_first=True, padding_value=0).type(torch.LongTensor).to(device)\n",
    "            output = model(char_sentences, sentences, lengths)\n",
    "            preds = output.argmax(dim=-1)\n",
    "            for idx, sentence in enumerate(batch):\n",
    "                pad_index = len(sentence)\n",
    "                predicted_tags.append(preds[idx, 0: pad_index])\n",
    "    return predicted_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Accuracy=0.9654\n",
      "Epoch 1: Validation Accuracy=0.9660\n",
      "processed 51578 tokens with 5942 phrases; found: 6417 phrases; correct: 4819.\n",
      "accuracy:  96.60%; precision:  75.10%; recall:  81.10%; FB1:  77.98\n",
      "              LOC: precision:  79.87%; recall:  83.18%; FB1:  81.49  1913\n",
      "             MISC: precision:  63.15%; recall:  65.62%; FB1:  64.36  958\n",
      "              ORG: precision:  62.91%; recall:  72.86%; FB1:  67.52  1553\n",
      "              PER: precision:  85.75%; recall:  92.78%; FB1:  89.13  1993\n",
      "Epoch 2: Train Accuracy=0.9662\n",
      "Epoch 2: Validation Accuracy=0.9672\n",
      "processed 51578 tokens with 5942 phrases; found: 6425 phrases; correct: 4881.\n",
      "accuracy:  96.72%; precision:  75.97%; recall:  82.14%; FB1:  78.94\n",
      "              LOC: precision:  79.27%; recall:  85.14%; FB1:  82.10  1973\n",
      "             MISC: precision:  66.74%; recall:  66.59%; FB1:  66.67  920\n",
      "              ORG: precision:  63.70%; recall:  73.68%; FB1:  68.33  1551\n",
      "              PER: precision:  86.57%; recall:  93.11%; FB1:  89.72  1981\n",
      "Epoch 3: Train Accuracy=0.9666\n",
      "Epoch 3: Validation Accuracy=0.9669\n",
      "processed 51578 tokens with 5942 phrases; found: 6386 phrases; correct: 4848.\n",
      "accuracy:  96.69%; precision:  75.92%; recall:  81.59%; FB1:  78.65\n",
      "              LOC: precision:  81.07%; recall:  83.23%; FB1:  82.14  1886\n",
      "             MISC: precision:  67.40%; recall:  66.16%; FB1:  66.78  905\n",
      "              ORG: precision:  61.85%; recall:  74.72%; FB1:  67.68  1620\n",
      "              PER: precision:  86.43%; recall:  92.67%; FB1:  89.44  1975\n",
      "Epoch 4: Train Accuracy=0.9672\n",
      "Epoch 4: Validation Accuracy=0.9677\n",
      "processed 51578 tokens with 5942 phrases; found: 6432 phrases; correct: 4894.\n",
      "accuracy:  96.77%; precision:  76.09%; recall:  82.36%; FB1:  79.10\n",
      "              LOC: precision:  80.32%; recall:  85.30%; FB1:  82.73  1951\n",
      "             MISC: precision:  66.77%; recall:  66.92%; FB1:  66.85  924\n",
      "              ORG: precision:  63.91%; recall:  73.83%; FB1:  68.51  1549\n",
      "              PER: precision:  85.66%; recall:  93.38%; FB1:  89.35  2008\n",
      "Epoch 5: Train Accuracy=0.9679\n",
      "Epoch 5: Validation Accuracy=0.9685\n",
      "processed 51578 tokens with 5942 phrases; found: 6397 phrases; correct: 4919.\n",
      "accuracy:  96.85%; precision:  76.90%; recall:  82.78%; FB1:  79.73\n",
      "              LOC: precision:  81.53%; recall:  85.30%; FB1:  83.37  1922\n",
      "             MISC: precision:  67.25%; recall:  67.25%; FB1:  67.25  922\n",
      "              ORG: precision:  64.58%; recall:  75.47%; FB1:  69.60  1567\n",
      "              PER: precision:  86.61%; recall:  93.38%; FB1:  89.86  1986\n",
      "Epoch 6: Train Accuracy=0.9682\n",
      "Epoch 6: Validation Accuracy=0.9686\n",
      "processed 51578 tokens with 5942 phrases; found: 6371 phrases; correct: 4911.\n",
      "accuracy:  96.86%; precision:  77.08%; recall:  82.65%; FB1:  79.77\n",
      "              LOC: precision:  81.88%; recall:  85.08%; FB1:  83.45  1909\n",
      "             MISC: precision:  67.72%; recall:  67.35%; FB1:  67.54  917\n",
      "              ORG: precision:  65.07%; recall:  74.87%; FB1:  69.63  1543\n",
      "              PER: precision:  86.06%; recall:  93.54%; FB1:  89.65  2002\n",
      "Epoch 7: Train Accuracy=0.9684\n",
      "Epoch 7: Validation Accuracy=0.9689\n",
      "processed 51578 tokens with 5942 phrases; found: 6356 phrases; correct: 4913.\n",
      "accuracy:  96.89%; precision:  77.30%; recall:  82.68%; FB1:  79.90\n",
      "              LOC: precision:  81.29%; recall:  85.85%; FB1:  83.51  1940\n",
      "             MISC: precision:  68.72%; recall:  66.49%; FB1:  67.59  892\n",
      "              ORG: precision:  65.04%; recall:  74.50%; FB1:  69.45  1536\n",
      "              PER: precision:  86.72%; recall:  93.59%; FB1:  90.03  1988\n",
      "Epoch 8: Train Accuracy=0.9684\n",
      "Epoch 8: Validation Accuracy=0.9688\n",
      "processed 51578 tokens with 5942 phrases; found: 6343 phrases; correct: 4893.\n",
      "accuracy:  96.88%; precision:  77.14%; recall:  82.35%; FB1:  79.66\n",
      "              LOC: precision:  80.71%; recall:  86.12%; FB1:  83.33  1960\n",
      "             MISC: precision:  68.33%; recall:  68.33%; FB1:  68.33  922\n",
      "              ORG: precision:  66.34%; recall:  71.74%; FB1:  68.94  1450\n",
      "              PER: precision:  85.48%; recall:  93.32%; FB1:  89.23  2011\n",
      "Epoch 9: Train Accuracy=0.9693\n",
      "Epoch 9: Validation Accuracy=0.9694\n",
      "processed 51578 tokens with 5942 phrases; found: 6370 phrases; correct: 4934.\n",
      "accuracy:  96.94%; precision:  77.46%; recall:  83.04%; FB1:  80.15\n",
      "              LOC: precision:  81.53%; recall:  86.28%; FB1:  83.84  1944\n",
      "             MISC: precision:  67.58%; recall:  66.92%; FB1:  67.25  913\n",
      "              ORG: precision:  66.01%; recall:  74.72%; FB1:  70.09  1518\n",
      "              PER: precision:  86.72%; recall:  93.92%; FB1:  90.17  1995\n",
      "Epoch 10: Train Accuracy=0.9694\n",
      "Epoch 10: Validation Accuracy=0.9697\n",
      "processed 51578 tokens with 5942 phrases; found: 6342 phrases; correct: 4919.\n",
      "accuracy:  96.97%; precision:  77.56%; recall:  82.78%; FB1:  80.09\n",
      "              LOC: precision:  82.37%; recall:  85.68%; FB1:  83.99  1911\n",
      "             MISC: precision:  67.17%; recall:  68.11%; FB1:  67.64  935\n",
      "              ORG: precision:  66.27%; recall:  74.57%; FB1:  70.18  1509\n",
      "              PER: precision:  86.41%; recall:  93.21%; FB1:  89.68  1987\n"
     ]
    }
   ],
   "source": [
    "task3_model_path = './blstm3.pt'\n",
    "def run_cnn_lstm_training():\n",
    "    word_vocab_size = len(vocab)\n",
    "    char_vocab_size = len(char_vocab)\n",
    "    word_embedding_dim = 101\n",
    "    char_embedding_dim = 60\n",
    "    char_output_dim = 30\n",
    "    num_layers = 1\n",
    "    hidden_dim = 256\n",
    "    linear_dim = 128\n",
    "    output_dim = len(labels_vocab)\n",
    "    lr = 1\n",
    "    batch_size = 64\n",
    "    num_epochs = 50\n",
    "    train_dataset = CNNNERDataset(char_train_X, train_X, train_Y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
    "    val_dataset = CNNNERDataset(char_train_X, dev_X, dev_Y)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
    "    \n",
    "    model = CharCNNLSTM(\n",
    "        word_vocab_size, \n",
    "        word_embedding_dim, \n",
    "        embeddings_weights, \n",
    "        char_vocab_size, \n",
    "        char_embedding_dim, \n",
    "        char_output_dim, \n",
    "        hidden_dim, \n",
    "        num_layers, \n",
    "        linear_dim, \n",
    "        output_dim)\n",
    "    model = torch.load(task3_model_path)\n",
    "    model.to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[10, 30], gamma=0.1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train(model, train_loader, optimizer, criterion)\n",
    "        predicted_train_tags = get_predicted_sentence_tags(cnn_predict(model, train_X, char_train_X, batch_size), labels_vocab)\n",
    "        train_accuracy = calculate_score(train_sentence_tags, predicted_train_tags)\n",
    "        predicted_dev_tags = get_predicted_sentence_tags(cnn_predict(model, dev_X, char_dev_X, batch_size), labels_vocab)\n",
    "        val_accuracy = calculate_score(dev_sentence_tags, predicted_dev_tags)\n",
    "        print(f'Epoch {epoch+1}: Train Accuracy={train_accuracy:.4f}')\n",
    "        print(f'Epoch {epoch+1}: Validation Accuracy={val_accuracy:.4f}')\n",
    "        predicted_dev_tags = get_predicted_sentence_tags(cnn_predict(model, dev_X, char_dev_X, batch_size), labels_vocab)\n",
    "        write_test_output('./dev3.out', dev_sentences, dev_sentence_tags, predicted_dev_tags)\n",
    "        !perl conll03eval.txt < dev3.out\n",
    "        scheduler.step()\n",
    "\n",
    "    # save model to file\n",
    "    torch.save(model, task3_model_path)\n",
    "        \n",
    "    return model\n",
    "  \n",
    "model = run_cnn_lstm_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 51578 tokens with 5942 phrases; found: 6121 phrases; correct: 5395.\n",
      "accuracy:  98.37%; precision:  88.14%; recall:  90.79%; FB1:  89.45\n",
      "              LOC: precision:  94.52%; recall:  92.87%; FB1:  93.68  1805\n",
      "             MISC: precision:  77.37%; recall:  86.01%; FB1:  81.46  1025\n",
      "              ORG: precision:  80.96%; recall:  83.37%; FB1:  82.15  1381\n",
      "              PER: precision:  93.09%; recall:  96.53%; FB1:  94.78  1910\n"
     ]
    }
   ],
   "source": [
    "predicted_dev_tags = get_predicted_sentence_tags(cnn_predict(model, dev_X, char_dev_X, batch_size), labels_vocab)\n",
    "write_test_output('./dev3.out', dev_sentences, dev_sentence_tags, predicted_dev_tags)\n",
    "!perl conll03eval.txt < dev3.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
